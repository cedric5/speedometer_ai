"""
Tests for Streamlit UI functionality
"""

import pytest
import tempfile
from pathlib import Path
from unittest.mock import patch, Mock, MagicMock
import pandas as pd
import streamlit as st

# Note: Testing Streamlit apps requires special handling
# We'll test the main functions and logic separately from the Streamlit components


class TestUIFunctions:
    """Test the core functions used by the UI"""
    
    def test_generate_summary_report(self):
        """Test summary report generation"""
        from speedometer_ai.ui import generate_summary_report
        
        results = [
            {'frame': 1, 'timestamp': 0.0, 'speed': 157, 'success': True},
            {'frame': 2, 'timestamp': 0.33, 'speed': 145, 'success': True},
            {'frame': 3, 'timestamp': 0.67, 'speed': None, 'success': False},
        ]
        
        stats = {
            'total_frames': 3,
            'successful_readings': 2,
            'success_rate': 66.7,
            'duration': 0.67,
            'min_speed': 145,
            'max_speed': 157,
            'avg_speed': 151.0
        }
        
        report = generate_summary_report(results, stats, "test_video.mp4")
        
        assert "SPEEDOMETER ANALYSIS REPORT" in report
        assert "Video: test_video.mp4" in report
        assert "Total frames: 3" in report
        assert "Successful readings: 2" in report
        assert "Success rate: 66.7%" in report
        assert "Speed range: 145-157 km/h" in report
        assert "Average speed: 151.0 km/h" in report
        assert "DETAILED TIMELINE:" in report
        assert "✓ 0.0s: 157 km/h" in report
        assert "✗ 0.7s: N/A km/h" in report
        assert "Generated by Speedometer AI v1.0.0" in report


class TestUILogic:
    """Test UI logic without Streamlit components"""
    
    @patch('speedometer_ai.ui.SpeedometerAnalyzer')
    @patch('speedometer_ai.ui.extract_frames_from_video')
    @patch('speedometer_ai.ui.validate_video_file')
    @patch('speedometer_ai.ui.check_ffmpeg_available')
    def test_analyze_video_function_success(self, mock_ffmpeg, mock_validate, 
                                          mock_extract, mock_analyzer_class):
        """Test the analyze_video function logic"""
        from speedometer_ai.ui import analyze_video
        
        # Setup mocks
        mock_ffmpeg.return_value = True
        mock_validate.return_value = True
        mock_extract.return_value = [Path("frame_001.png")]
        
        mock_analyzer = Mock()
        mock_analyzer.analyze_video_frames.return_value = [
            {'speed': 157, 'success': True, 'timestamp': 0.0}
        ]
        mock_analyzer.get_statistics.return_value = {'success_rate': 100.0}
        mock_analyzer_class.return_value = mock_analyzer
        
        # Mock Streamlit components
        with patch('streamlit.progress') as mock_progress, \
             patch('streamlit.empty') as mock_status, \
             patch('streamlit.success') as mock_success, \
             patch('streamlit.rerun') as mock_rerun:
            
            mock_progress_bar = Mock()
            mock_status_text = Mock()
            mock_progress.return_value = mock_progress_bar
            mock_status.return_value = mock_status_text
            
            # Call the function
            video_path = Path("test_video.mp4")
            analyze_video(video_path, "test_api_key", 3.0, 1.0, False, False)
            
            # Verify mocks were called
            mock_extract.assert_called_once()
            mock_analyzer_class.assert_called_once_with("test_api_key")
            mock_analyzer.analyze_video_frames.assert_called_once()
            mock_success.assert_called()
            mock_rerun.assert_called_once()
    
    @patch('speedometer_ai.ui.SpeedometerAnalyzer')
    @patch('speedometer_ai.ui.extract_frames_from_video')
    @patch('speedometer_ai.ui.validate_video_file')
    @patch('speedometer_ai.ui.check_ffmpeg_available')
    def test_analyze_video_function_error(self, mock_ffmpeg, mock_validate, 
                                        mock_extract, mock_analyzer_class):
        """Test the analyze_video function with error"""
        from speedometer_ai.ui import analyze_video
        
        # Setup mocks to raise an error
        mock_ffmpeg.return_value = True
        mock_validate.return_value = True
        mock_extract.side_effect = Exception("Test error")
        
        # Mock Streamlit components
        with patch('streamlit.progress') as mock_progress, \
             patch('streamlit.empty') as mock_status, \
             patch('streamlit.error') as mock_error:
            
            mock_progress_bar = Mock()
            mock_status_text = Mock()
            mock_progress.return_value = mock_progress_bar
            mock_status.return_value = mock_status_text
            
            # Call the function
            video_path = Path("test_video.mp4")
            analyze_video(video_path, "test_api_key", 3.0, 1.0, False, False)
            
            # Verify error was displayed
            mock_error.assert_called_once()
            error_msg = mock_error.call_args[0][0]
            assert "Error during analysis: Test error" in error_msg
    
    def test_display_results_function(self):
        """Test the display_results function logic"""
        from speedometer_ai.ui import display_results
        
        analysis_data = {
            'results': [
                {'frame': 1, 'timestamp': 0.0, 'speed': 157, 'success': True, 'filename': 'frame_001.png'},
                {'frame': 2, 'timestamp': 0.33, 'speed': 145, 'success': True, 'filename': 'frame_002.png'},
                {'frame': 3, 'timestamp': 0.67, 'speed': None, 'success': False, 'filename': 'frame_003.png'},
            ],
            'stats': {
                'total_frames': 3,
                'successful_readings': 2,
                'success_rate': 66.7,
                'duration': 0.67,
                'min_speed': 145,
                'max_speed': 157
            },
            'video_name': 'test_video.mp4'
        }
        
        # Mock Streamlit components
        with patch('streamlit.columns') as mock_columns, \
             patch('streamlit.metric') as mock_metric, \
             patch('streamlit.subheader') as mock_subheader, \
             patch('streamlit.plotly_chart') as mock_plotly, \
             patch('streamlit.dataframe') as mock_dataframe, \
             patch('streamlit.download_button') as mock_download:
            
            # Mock columns
            mock_col1, mock_col2, mock_col3, mock_col4 = [Mock() for _ in range(4)]
            mock_columns.return_value = [mock_col1, mock_col2, mock_col3, mock_col4]
            
            # Call the function
            display_results(analysis_data)
            
            # Verify components were called
            mock_metric.assert_called()  # Should be called multiple times
            mock_subheader.assert_called()
            mock_plotly.assert_called_once()
            mock_dataframe.assert_called_once()
            mock_download.assert_called()  # Should be called for CSV and report downloads


class TestUIIntegration:
    """Integration tests for UI components"""
    
    def test_ui_imports(self):
        """Test that UI module imports correctly"""
        try:
            from speedometer_ai import ui
            assert hasattr(ui, 'main')
            assert hasattr(ui, 'analyze_video')
            assert hasattr(ui, 'display_results')
            assert hasattr(ui, 'generate_summary_report')
        except ImportError as e:
            pytest.fail(f"UI module import failed: {e}")
    
    def test_ui_dependencies(self):
        """Test that all UI dependencies are available"""
        try:
            import streamlit
            import plotly.express
            import plotly.graph_objects
            import pandas
        except ImportError as e:
            pytest.fail(f"UI dependency missing: {e}")
    
    @patch('streamlit.set_page_config')
    @patch('streamlit.title')
    @patch('streamlit.markdown')
    @patch('streamlit.sidebar')
    @patch('streamlit.columns')
    @patch('streamlit.header')
    @patch('streamlit.file_uploader')
    @patch('streamlit.info')
    def test_main_ui_structure(self, mock_info, mock_file_uploader, mock_header,
                              mock_columns, mock_sidebar, mock_markdown, 
                              mock_title, mock_page_config):
        """Test that main UI function sets up correctly"""
        from speedometer_ai.ui import main
        
        # Mock file uploader to return None (no file uploaded)
        mock_file_uploader.return_value = None
        
        # Mock sidebar context manager
        mock_sidebar_context = Mock()
        mock_sidebar.return_value.__enter__ = Mock(return_value=mock_sidebar_context)
        mock_sidebar.return_value.__exit__ = Mock(return_value=None)
        
        # Mock columns
        mock_col1, mock_col2 = Mock(), Mock()
        mock_columns.return_value = [mock_col1, mock_col2]
        
        # Call main function
        main()
        
        # Verify basic setup calls
        mock_page_config.assert_called_once()
        mock_title.assert_called_once_with("🚗 Speedometer AI")
        mock_markdown.assert_called_once_with("AI-powered speedometer reading from dashboard video")
        mock_info.assert_called_once()  # Should show the "upload and analyze" message


class TestUIEndToEnd:
    """End-to-end tests for UI functionality"""
    
    def test_ui_session_state(self):
        """Test UI session state management"""
        # This would require a more complex setup with Streamlit testing framework
        # For now, we'll test that the session state keys are handled correctly
        
        # Mock session state
        mock_session_state = {}
        
        with patch('streamlit.session_state', mock_session_state):
            from speedometer_ai.ui import display_results
            
            # Test that display_results handles missing session state gracefully
            analysis_data = {
                'results': [],
                'stats': {'total_frames': 0},
                'video_name': 'test.mp4'
            }
            
            # This should not raise an error even without session state setup
            with patch('streamlit.columns'), \
                 patch('streamlit.metric'), \
                 patch('streamlit.subheader'), \
                 patch('streamlit.dataframe'), \
                 patch('streamlit.download_button'):
                
                try:
                    display_results(analysis_data)
                except Exception as e:
                    pytest.fail(f"display_results failed with session state: {e}")
    
    def test_ui_error_handling(self):
        """Test UI error handling"""
        from speedometer_ai.ui import analyze_video
        
        # Test with invalid inputs
        with patch('streamlit.progress'), \
             patch('streamlit.empty'), \
             patch('streamlit.error') as mock_error:
            
            # This should handle the error gracefully
            analyze_video(Path("nonexistent.mp4"), "", 3.0, 1.0, False, False)
            
            # Should have called error display
            mock_error.assert_called()


# Specs for UI End-to-End Testing
class TestUISpecs:
    """
    Specifications for UI End-to-End Testing
    
    These tests define the expected behavior of the UI from a user perspective.
    They can be used as a guide for manual testing or automated browser testing.
    """
    
    def test_ui_basic_workflow_spec(self):
        """
        SPEC: Basic UI Workflow
        
        User Story: As a user, I want to upload a video and get speed analysis results
        
        Expected Flow:
        1. User opens UI at http://localhost:8501
        2. UI displays title "🚗 Speedometer AI" 
        3. UI shows sidebar with configuration options
        4. User enters API key in sidebar
        5. User uploads video file using file uploader
        6. User clicks "🔍 Analyze Video" button
        7. UI shows progress bar during analysis
        8. UI displays results with charts and metrics
        9. User can download CSV and report files
        
        Success Criteria:
        - All UI elements are visible and functional
        - File upload accepts mp4, avi, mov, mkv files
        - Progress bar shows analysis progress
        - Results display speed chart and timeline
        - Download buttons generate valid files
        """
        assert True  # This is a specification, not a runnable test
    
    def test_ui_error_handling_spec(self):
        """
        SPEC: UI Error Handling
        
        User Story: As a user, I want clear error messages when something goes wrong
        
        Expected Error Scenarios:
        1. No API key provided -> Show "Please enter your Gemini API key"
        2. Invalid video file -> Show "Invalid video file"
        3. FFmpeg not found -> Show "FFmpeg not found. Please install FFmpeg"
        4. API error during analysis -> Show specific error message
        5. File upload fails -> Show upload error message
        
        Success Criteria:
        - All errors display clear, actionable messages
        - UI remains functional after errors
        - User can retry after fixing issues
        """
        assert True  # This is a specification, not a runnable test
    
    def test_ui_configuration_spec(self):
        """
        SPEC: UI Configuration Options
        
        User Story: As a user, I want to configure analysis settings
        
        Expected Configuration Options:
        1. API Key input (password field)
        2. Frames per second slider (1.0-10.0, default 3.0)
        3. API delay slider (0.5-3.0, default 1.0)
        4. Keep frames checkbox (default False)
        5. Verbose output checkbox (default False)
        
        Success Criteria:
        - All settings are saved and applied to analysis
        - Settings persist during the session
        - Default values are reasonable
        - Sliders have appropriate ranges and steps
        """
        assert True  # This is a specification, not a runnable test
    
    def test_ui_results_display_spec(self):
        """
        SPEC: Results Display
        
        User Story: As a user, I want to see comprehensive analysis results
        
        Expected Results Display:
        1. Summary metrics (4 columns):
           - Total Frames
           - Success Rate (%)
           - Speed Range (km/h)
           - Duration (s)
        2. Interactive speed chart using Plotly
        3. Detailed results table with:
           - Status (✅/❌)
           - Time (s)
           - Speed (km/h)
           - Filename
        4. Download section with:
           - CSV download button
           - Summary report download button
        
        Success Criteria:
        - All metrics display correctly
        - Chart is interactive and shows speed over time
        - Table shows all frame results
        - Downloads generate valid files with correct content
        """
        assert True  # This is a specification, not a runnable test
    
    def test_ui_performance_spec(self):
        """
        SPEC: UI Performance
        
        User Story: As a user, I want the UI to be responsive and efficient
        
        Expected Performance:
        1. Initial load time < 3 seconds
        2. File upload processes immediately
        3. Progress updates every frame analyzed
        4. Results display within 1 second of completion
        5. Chart rendering < 2 seconds
        6. Download generation < 1 second
        
        Success Criteria:
        - UI remains responsive during analysis
        - Progress bar updates smoothly
        - No blocking operations on UI thread
        - Memory usage remains reasonable
        """
        assert True  # This is a specification, not a runnable test